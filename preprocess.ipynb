{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합(vocabulary)의 크기 : 8648\n",
      "등장 빈도가 2번 이하인 희귀 단어의 수: 4170\n",
      "단어 집합에서 희귀 단어의 비율: 48.21924144310824\n",
      "전체 등장 빈도에서 희귀 단어 등장 빈도 비율: 3.497673817523908\n",
      "단어 집합의 크기 : 4478\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "korean_air_data = pd.read_csv(\"korean_air_data/korean_air_2020_01.csv\")\n",
    "\n",
    "korean_air_data \n",
    "\n",
    "korean_air_data.columns = [\"a\",\"title\",\"text\",\"editor\",\"times\",\"date\"]\n",
    "\n",
    "del korean_air_data[\"a\"]\n",
    "\n",
    "# na값 제거\n",
    "data = korean_air_data.dropna()\n",
    "\n",
    "# 불용어 지정\n",
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','박정환','도','을','것','를','등','으로','자','에','등','와','한','하다''것','\\n','로']\n",
    "\n",
    "# 한글 외의 모든 데이터 제거 : 기업명이 영어로 되어있는 경우를 어떻게 처리 해야할까?\n",
    "data = data.text.str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\")\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "okt = Okt()\n",
    "X_train = []\n",
    "\n",
    "for sen in data :\n",
    "    temp_X = []\n",
    "    \n",
    "    # 토큰화\n",
    "    temp_X = okt.morphs(sen, stem = True) \n",
    "    \n",
    "    # 불용어 제거\n",
    "    temp_X = [word for word in temp_X if not word in stopwords] \n",
    "    \n",
    "    X_train.append(temp_X)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "threshold = 3\n",
    "total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "rare_cnt = 0 # 등장 빈도수가 threshold보다 작은 단어의 개수를 카운트\n",
    "total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "rare_freq = 0 # 등장 빈도수가 threshold보다 작은 단어의 등장 빈도수의 총 합\n",
    "\n",
    "# 단어와 빈도수의 쌍(pair)을 key와 value로 받는다.\n",
    "for key, value in tokenizer.word_counts.items():\n",
    "    total_freq = total_freq + value\n",
    "\n",
    "    # 단어의 등장 빈도수가 threshold보다 작으면\n",
    "    if(value < threshold):\n",
    "        rare_cnt = rare_cnt + 1\n",
    "        rare_freq = rare_freq + value\n",
    "\n",
    "print('단어 집합(vocabulary)의 크기 :',total_cnt)\n",
    "print('등장 빈도가 %s번 이하인 희귀 단어의 수: %s'%(threshold - 1, rare_cnt))\n",
    "print(\"단어 집합에서 희귀 단어의 비율:\", (rare_cnt / total_cnt)*100)\n",
    "print(\"전체 등장 빈도에서 희귀 단어 등장 빈도 비율:\", (rare_freq / total_freq)*100)\n",
    "\n",
    "# 전체 단어 개수 중 빈도수 2이하인 단어 개수는 제거.\n",
    "vocab_size = total_cnt - rare_cnt\n",
    "print('단어 집합의 크기 :',vocab_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = []\n",
    "str = ''\n",
    "\n",
    "for k in range(0,len(X_train)):\n",
    "    for i in range(0,len(X_train[k])):\n",
    "        str = str + X_train[k][i] + ' '\n",
    "    nouns.append(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_df = pd.DataFrame()\n",
    "\n",
    "tfIdfVectorizer=TfidfVectorizer(use_idf=True)\n",
    "tfidf = tfIdfVectorizer.fit_transform(nouns)\n",
    "\n",
    "for i in range(0,len(X_train)):\n",
    "    df = pd.DataFrame(tfidf[i].T.toarray(), index=tfIdfVectorizer.get_feature_names(), columns=[\"text{}\".format(i)])\n",
    "    tfidf_df = pd.concat([tfidf_df,df],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf = []\n",
    "\n",
    "for i in range(0,len(X_train)):\n",
    "    idx = tfidf_df['text{}'.format(i)] > 0.01\n",
    "    result = tfidf_df[idx].index.tolist()\n",
    "    X_train_tfidf.append(result)\n",
    "\n",
    "len(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.corpus import kolaw\n",
    "from konlpy.tag import Okt\n",
    "from nltk.collocations import BigramAssocMeasures\n",
    "from nltk.collocations import TrigramAssocMeasures\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from collections import Counter\n",
    "\n",
    "ngram = [(BigramAssocMeasures(),BigramCollocationFinder),\n",
    "         (TrigramAssocMeasures(),TrigramCollocationFinder)]\n",
    "\n",
    "okt= Okt()# loading tagger\n",
    "nouns = []\n",
    "for sen in data:\n",
    "    token = okt.morphs(sen, stem = True) # get nouns\n",
    "    token = [word for word in token if not word in stopwords] \n",
    "    nouns.append(token)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "founds_from_4measure = []\n",
    "X_train_ngram = []\n",
    "\n",
    "for i in range(0,len(nouns)):\n",
    "    for measure, finder in ngram:\n",
    "        finder = finder.from_words(nouns[i])\n",
    "        founds = finder.nbest(measure.pmi, 10)       # pmi - 상위 30개 추출\n",
    "        founds += finder.nbest(measure.chi_sq, 10)   # chi_sq - 상위 30개 추출\n",
    "        founds += finder.nbest(measure.mi_like, 10)  # mi_like - 상위 30개 추출\n",
    "        founds += finder.nbest(measure.jaccard, 10)  # jaccard - 상위 30개 추출\n",
    "\n",
    "        founds_from_4measure += founds\n",
    "\n",
    "        collocations = [' '.join(collocation) for collocation in founds_from_4measure]\n",
    "        collocations = [(w) for w,f in Counter(collocations).most_common() if f > 2]\n",
    "        \n",
    "    X_train_ngram.append(collocations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pre = []\n",
    "\n",
    "for i in range(0,len(X_train)):\n",
    "    X_train_pre.append(X_train_g[i] + X_train_ngram[i])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
